{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 9 Exercises - Salinee Kingbaisomboon.ipynb","provenance":[{"file_id":"1dJHR3g1oicYCvt5lJQySI6pIoow8kDXB","timestamp":1615360746645}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"db8puyKPtXRS"},"source":["# **RBOT 240 - Week 9 Exercises**\n","\n","Complete 3 of the following exercises.\n","\n","The numbered exercises below are from the Sutton & Barto book [Reinforcement Learning: An Introduction](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf) from this week's reading material.  Please see the book for more context on these problems."]},{"cell_type":"markdown","metadata":{"id":"XLUgT2Ny2-0T"},"source":["---\r\n","\r\n","# <font color=\"blue\">My selected three questions are:</font>\r\n","\r\n","<font color=\"blue\">- <b>Discussion Exercise 3.1</b></font>\r\n","\r\n","<font color=\"blue\">- <b>Discussion Exercise 3.2</b></font>\r\n","\r\n","<font color=\"blue\">- <b>Programming Exercise P1</b></font>\r\n","\r\n","---"]},{"cell_type":"markdown","metadata":{"id":"1g-bdOvRxKXV"},"source":["## Problem Exercises"]},{"cell_type":"markdown","metadata":{"id":"ODlOCQvxzpZb"},"source":["**(Exercise 3.14)** The Bellman equation (3.14) must hold for each state for the value function $v_\\pi$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)"]},{"cell_type":"markdown","metadata":{"id":"oRmX1h6aNJwK"},"source":["**(Exercise 4.3)** What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_\\pi$ and its successive approximation by a sequence of functions $q_0$, $q_1$, $q_2$,...?"]},{"cell_type":"markdown","metadata":{"id":"Twu0e_1WQOys"},"source":["**(Exercise 4.5)** How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."]},{"cell_type":"markdown","metadata":{"id":"tuxL-C3cRAQg"},"source":["**(Exercise 4.10)** What is the analog of the value iteration update (4.10) for action values, $q_{k+1}(s,a)$?"]},{"cell_type":"markdown","metadata":{"id":"0BTMIRsERQ9U"},"source":["## Discussion Exercises"]},{"cell_type":"markdown","metadata":{"id":"ZgiSYKVJ0LwZ"},"source":["**<font color=\"blue\">(Exercise 3.1)** Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.</font>\n","\n","<font color=\"green\"><u><b>Answer</b></u></font>\n","\n","1.   **Finance**: Based on this paper, \"***Using Markov Decision Processes to Solve a Portfolio Allocation Problem*** [1]\", **MDP** can be used to learn the  optimal time to buy or sell stocks based on the current available information. \n","\n","    In this case the **state** would be the size(amount of dollars) and direction (+/-) of the real-time return of the stock.\n","    \n","    The **action** might be the quantities (how much of a stock to buy or sell) and whether to buy or sell. \n","\n","    The **reward** can be the corresponding profit or loss from this action. \n","2.   **Queues**: Based on this article, \"***Application of reinforcement learning to control traffic signals*** [2]\", **MDP** can be used for ***Traffic Control***. For example, we want to decide the duration of traffic lights in an intersection maximizing the number cars passing the intersection without stopping.\n","\n","    In this case the **state** is represented of a traffic environments such as color of the traffic light (red, green) in each directions, the duration of the traffic light and number of cars.\n","\n","    The **action** is whether or not to change the traffic light.\n","\n","    The **reward** is the number of cars passing the intersection in the next time step minus some sort of discount for the traffic blocked in the other direction.\n","3.   **Robotics**: Robot can learn of how to escape a maze or any kind of navigation solver by using **MDP**.[3] \n","\n","    The **state** could be the robots position and its orientation in the maze. \n","    \n","    The **actions** could be one of the following: to move forward, backwards, or turn 90 degrees. \n","    \n","    The **reward** could be <font color=\"red\">-1</font> if the robot hit a wall and <font color=\"green\">+1</font> if the robot find an exit of the maze.\n","\n","[1] https://cs.brown.edu/research/pubs/theses/ugrad/2005/dbooksta.pdf\n","\n","[2] https://blogs.sas.com/content/subconsciousmusings/2020/12/16/reinforcement-learning-neurips/\n","\n","[3] https://web.cs.dal.ca/~tt/robotics/415511_MDP&RL.pdf\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7EPzbrSLv5Si"},"source":["**<font color=\"blue\">(Exercise 3.2)** Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?</font>\r\n","\r\n","<font color=\"green\"><u><b>Answer</b></u></font>\r\n","\r\n","Based on my current knowledge, I can come up with one use case. It's not really an exception but rather not that <font color=\"red\">**optimization**</font> to use a **MDP**. One of that is **Trajectory planning**.\r\n","\r\n","**Trajectory planning** is moving from point A to point B while avoiding collisions over time. This can be computed in both discrete and continuous methods. Trajectory planning is a major area in robotics as it gives way to autonomous vehicles.[1]\r\n","\r\n","At first, it seems this scenario can be solved by using **MDP**. If it's a long one, I think it will be **Navigation** rather than **Trajectory**. \r\n","\r\n","One of a popular case would be **Trajectory for Robot Maniputator** such as industrail robot arms.\r\n","\r\n","<image src=\"https://www.mathworks.com/responsive_image/165/120/0/0/0/cache/matlabcentral/mlc-downloads/downloads/0b300068-f139-417c-802c-5441b69b8c90/34c167e4-8005-4f71-9e8b-3693e5772d14/images/screenshot.png\" />\r\n","\r\n","Based on an above picture, using **MDP** is probably too overkilled. Based on this paper, \"Calculation Methodology for Trajectory Planning\r\n","of a 6 axis manipulator arm [2]\". I just skimmed through this paper and see that with the use of **Polynomial Method**, **Motion Kinematics Principle** and **Joint coordination** to establish the **Motion Equation** is suffix.\r\n","\r\n","**MDP** would be too much for this scenario since the real world is under the known and controlled environment (in the cage or designate area inside a warehouse). There is not so much **undertainty** or any **complexity** in the real world that will effect the motion of the robot arms. \r\n","\r\n","We don't need to find an optimal path of the robot arm by finding the best path through the **value iteration policy** until it converge but instead using an equation to solve for that path in one go.\r\n","\r\n","Based on this *StackExchange* [3], time complexity of the **value iteration policy** is O(|S×A|) for each update to a single V(s) estimate is $O(|S|^2|A|)$ while for the **Polynomial function** is $((a_{n}x+a{n-1})x+...)x+a_{0}$ using *Horner's Rule [4]*. \r\n","\r\n","By just looking the the time complexity of both cases from above, we can't say for sure which one is bigger. But, one thing we can say is that if the **state space $(|S|)$** is large, the time complexity of the **value iteration policy** will be fore sure much large than the **Polynomial function**.\r\n","\r\n","[1] https://en.wikibooks.org/wiki/Robotics/Navigation/Trajectory_Planning#:~:text=Trajectory%20planning%20is%20moving%20from,gives%20way%20to%20autonomous%20vehicles.\r\n","\r\n","[2] http://annals.fih.upt.ro/pdf-full/2018/ANNALS-2018-3-03.pdf\r\n","\r\n","[3] https://ai.stackexchange.com/questions/9019/what-is-the-time-complexity-of-the-value-iteration-algorithm\r\n","\r\n","[4] https://mathworld.wolfram.com/HornersRule.html"]},{"cell_type":"markdown","metadata":{"id":"oeb0JFYMS68e"},"source":["**(Exercise 3.7)** Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively\n","communicated to the agent what you want it to achieve?"]},{"cell_type":"markdown","metadata":{"id":"UA86yWCbOyMv"},"source":["## Programming Exercises\n"]},{"cell_type":"markdown","metadata":{"id":"Ljce6k1lF2HZ"},"source":["**<font color=\"blue\">(Exercise P1)** Adapt one of your solutions for assignment 5 (either policy iteration or value iteration, your choice) to another [toy text](https://gym.openai.com/envs/#toy_text) gym environment of your choice.</font>\r\n","\r\n","<font color=\"green\"><u><b>Answer</b></u></font>\r\n","\r\n","Below code was adapted from assignment 5's solution but implement in numpy for the **Taxi-v3** toy text. The task is to Teach a Taxi to pick up and drop off passengers at the right locations with Reinforcement Learning by implementing a **Policy Iteration** to estimate $\\pi_{*}$."]},{"cell_type":"code","metadata":{"id":"FcXQfr5hX6Tx","executionInfo":{"status":"ok","timestamp":1615830970947,"user_tz":360,"elapsed":244,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}}},"source":["import gym\r\n","import numpy as np \r\n","import torch\r\n","\r\n","import warnings\r\n","warnings.filterwarnings('ignore')\r\n","\r\n","# Set the environment\r\n","env = gym.make('Taxi-v3')   \r\n","env._max_episode_steps = 50\r\n","env.reset()\r\n","\r\n","# Implement the policy evluation algorithm here given a policy and a complete model of the environment.\r\n","def policy_evaluation(policy, env, gamma=0.9, theta=1e-8):\r\n","    # Start with an all zero value function\r\n","    V = np.zeros(env.nS)\r\n","    while True:\r\n","        delta = 0\r\n","        for s in range(env.nS):\r\n","            v = 0\r\n","            for a in range(env.nA):\r\n","                for prob, nextState, reward, done in env.P[s][a]:\r\n","                    v+=policy[s][a] * prob * (reward + gamma*V[nextState])\r\n","            \r\n","            delta = max(delta, np.abs(v-V[s]))\r\n","            V[s] = v\r\n","                \r\n","        if delta < theta:\r\n","            break\r\n","    \r\n","    return np.array(V)\r\n","\r\n","# Implement the Policy Improvement Algorithm here which iteratively evaluates and improves a policy until an optimal policy is found.\r\n","def policy_improvement(env, policy, policy_eval_fn=policy_evaluation, gamma=0.9):\r\n","\r\n","    while True:  \r\n","        # Start with all zero action value  \r\n","        action_value = np.zeros([env.nS, env.nA])\r\n","\r\n","        V = policy_eval_fn(policy, env, gamma)\r\n","        policy_stable = True\r\n","        \r\n","        for s in range(env.nS):\r\n","            old_action = np.argmax(policy[s])\r\n","\r\n","            # Implement the function to calculate the value for all actions in a given state.\r\n","            for a in range(env.nA):\r\n","              for prob, nextState, reward, done in env.P[s][a]:\r\n","                action_value[s][a] += prob * (reward + gamma * V[nextState])\r\n","\r\n","            new_action = np.argmax(action_value[s])\r\n","\r\n","            if old_action != new_action:\r\n","                policy_stable = False\r\n","                        \r\n","            policy[s] = np.zeros([env.nA])\r\n","            policy[s][new_action] = 1\r\n","\r\n","        if policy_stable:\r\n","            return policy, V\r\n","    \r\n","    return policy, np.zeros(env.env.nS)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"ic6AQUoZYIao","executionInfo":{"status":"ok","timestamp":1615830992279,"user_tz":360,"elapsed":18610,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}}},"source":["# Initialize arbitrary: 25% each action\r\n","policy = np.ones([env.nS, env.nA]) / env.nA\r\n","\r\n","policy_pi, value_pi = policy_improvement(env, policy, gamma=0.9)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVI768WQYUex","executionInfo":{"status":"ok","timestamp":1615830883351,"user_tz":360,"elapsed":266,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}},"outputId":"c82a72d9-6cb9-410a-e2f6-6f94d328e208"},"source":["import numpy as np\r\n","np.set_printoptions(threshold=np.inf)\r\n","\r\n","# Show all value in numpy array of the optimal policy (policy pi)\r\n","print(policy_pi)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6F5EGVX4eaVX"},"source":["**(Exercise P2)** The official PyTorch examples includes [an implementation of REINFORCE](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py), which represents the agent policy using a differentiable function (a neural network).  Adapt this implementation to the FrozenLake environment problem.\n","\n","\n","\n"]}]}