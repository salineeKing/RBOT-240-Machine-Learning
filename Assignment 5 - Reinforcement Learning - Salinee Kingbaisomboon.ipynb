{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 5 - Reinforcement Learning - Salinee Kingbaisomboon.ipynb","provenance":[{"file_id":"1o_i1joGXQzXsSxjgnBAE7utrz_zs3RC5","timestamp":1615360740084},{"file_id":"1mygshJPOENYGcbFI209YD1r_L97F90tg","timestamp":1615344554397},{"file_id":"1D1wYKnjnotZ4d9oRigZCE8mADtEYdr_l","timestamp":1615060187411}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WdYmUoQ-DljB"},"source":["# Assignment 5 - Reinforcement Learning\n","\n","### RBOT240 - Machine Learning\n","\n","#### Instructor: Daniel Pineo &copy; 2021\n","\n","#### Submitted: Salinee Kingbaisomboon"]},{"cell_type":"markdown","metadata":{"id":"dUXTD2SApYsW"},"source":["# FrozenLake\n","\n","In this assignment we're going to be looking at the FrozenLake problem in the OpenAI Gym simulation envronment, described below. \n","\n","![image](https://drive.google.com/uc?export=view&id=1ExLANuLFc4LW8g84_1gKJj4gClvQpEkW)\n","\n","The environment has 16 states and 4 actions:\n","\n","![image](https://drive.google.com/uc?export=view&id=1Y6B_jH9eNibXL_xMHMEMl2ID1tvj_Hnx)\n","\n","Documentation for the OpenAI FrozenLake environment can be found [here](https://gym.openai.com/envs/FrozenLake-v0/).\n","\n","We'll be using this environment to experiment with our first reinforcement algorithms: Policy Iteration and Value Iteration."]},{"cell_type":"code","metadata":{"id":"NUseMFyNeKoi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615951787589,"user_tz":360,"elapsed":5243,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}},"outputId":"406c4e67-aa39-41ee-ed31-d80fdc200b1f"},"source":["import gym\n","import numpy as np\n","import torch\n","\n","# The grid can be 4x4 or 8x8, but we'll start with the 4x4 for simplicity.\n","grid_dim = 4     # can be 4 or 8\n","map_name = f\"{grid_dim}x{grid_dim}\"\n","\n","# Gamma is the discount rate for future rewards\n","gamma = 0.9\n","\n","# Setting is_slippery to True results in a 33% chance to move \n","# in a direction 90 degrees from the intended direction when\n","# taking an action.\n","env = gym.make(\"FrozenLake-v0\", map_name=map_name, is_slippery=False).env\n","env.render()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MTHwkIfsTj2F","executionInfo":{"status":"ok","timestamp":1615951838356,"user_tz":360,"elapsed":298,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}}},"source":["# Helper function to visualize a policy.  A policy is a probability \n","# distribution over possible actions for each state.  However, we will\n","# only show the action associated with the highest-value for each state.\n","\n","def draw_policy(policy):\n","    action_symbols = \"<v>^\"\n","    best_policy = np.argmax(policy, axis=-1).reshape([-1, grid_dim])\n","    policy_arrows = np.vectorize(lambda x: action_symbols[x])(best_policy)\n","    print(np.where(env.desc == b'F', policy_arrows, env.desc))"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Wdp6Qcig4Vi"},"source":["# Policy Iteration\n","\n","In this section, we'll be implementing the policy iteration algorithm (Sutton & Barto, section 4.3).  One of the steps of this algorithm that we'll be implementing is the policy evaluation, in which the value function is calculated for a given policy.\n","\n","![image](https://drive.google.com/thumbnail?id=113NOzgN9o9ITaW3t0wHENT0Sb1pXUTzg&sz=w800)"]},{"cell_type":"code","metadata":{"id":"yDbQzN3dejp9","executionInfo":{"status":"ok","timestamp":1615951872230,"user_tz":360,"elapsed":634,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}}},"source":["def policy_evaluation(env, policy, gamma=1.0):\n","    state_value = torch.zeros(env.nS)  # state-value function.  initialize arbitrarily\n","    theta=1e-8\n","    while True:\n","        state_value_previous = state_value.detach().clone()\n","\n","        # Calculate state_value, the state-value function according to the formula \n","        # above.  It should be implemented as an array of shape (num_states,)\n","\n","        # hint: generally like to do use vector/matrix operations when I can, \n","        # but the OpenAI environment doesn't have the data stored that way, so\n","        # I used a bunch of nested loops to accumulate over the sums\n","\n","        # hint: some of the information you need is in env:\n","        #   env.nS = number of states\n","        #   env.nA = number of actions\n","        #   env.P[state][action] = (transition_probability, next_state, reward, is_done)\n","\n","        ################################################\n","        ####### Place your implementation here #########\n","        ################################################\n","        delta = 0\n","        for s in range(env.nS):\n","          v = 0\n","          for a, action_prob in enumerate(policy[s]):\n","            for prob, next_state, reward, done in env.P[s][a]:\n","              v += action_prob * prob * (reward + gamma * state_value[next_state])\n","          delta = max(delta, abs(v-state_value[s]))\n","          state_value[s] = v\n","        if delta < theta:\n","          break\n","\n","        if torch.allclose(state_value_previous, state_value): \n","            break\n","\n","    return state_value\n","\n","# Test the random policy (25% chance to go in each direction)\n","assert torch.allclose(\n","    policy_evaluation(env, torch.ones([env.nS, env.nA]) / env.nA), \n","    torch.tensor([\n","            0.0139398 , 0.01163093, 0.02095299, 0.01047649,\n","            0.01624867, 0.        , 0.04075154, 0.        ,\n","            0.0348062 , 0.08816993, 0.14205316, 0.        ,\n","            0.        , 0.17582037, 0.43929118, 0.        \n","        ])\n","    , atol=1e-6)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IMb0muml3Qj-"},"source":["Next we'll look at how the policy evaluation is used in the subsequent policy improvement step.\n","\n","![image](https://drive.google.com/thumbnail?id=1p0q0MI95JZyg_eLonJJZL9mlP1Dd0kEg&sz=w800)"]},{"cell_type":"code","metadata":{"id":"twWqBGDj8WD-","executionInfo":{"status":"ok","timestamp":1615951972684,"user_tz":360,"elapsed":330,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}}},"source":["def policy_improvement(env, state_value, gamma, debug=False):\n","\n","    # Calculate action_value, the action-value function according to the formula above.\n","    # It should be implemented as an array of shape (num_states, num_actions)\n","\n","    # (hint: same hints as above)\n","\n","    ################################################\n","    ####### Place your implementation here #########\n","    ################################################\n","\n","    # Initialize variables\n","    policy = torch.zeros([env.nS, env.nA])\n","    action_value = torch.zeros([env.nS, env.nA])\n","\n","    for s in range(env.nS):\n","      for a, action_prob in enumerate(policy[s]):\n","            for prob, next_state, reward, done in env.P[s][a]:\n","              action_value[s][a] += prob * (reward + gamma * state_value[next_state])\n","\n","    if debug:\n","        assert torch.allclose(\n","            action_value,\n","            torch.tensor([\n","                [0.4466, 0.2767, 0.6914, 0.4466],\n","                [0.4466, 0.5707, 0.0796, 0.6914],\n","                [0.6914, 0.4411, 0.1188, 0.0796],\n","                [0.0796, 0.8068, 0.1188, 0.1188],\n","                [0.2767, 0.4101, 0.5707, 0.4466],\n","                [0.5707, 0.5707, 0.5707, 0.5707],\n","                [0.5707, 0.3140, 0.8068, 0.0796],\n","                [0.8068, 0.8068, 0.8068, 0.8068],\n","                [0.4101, 0.0201, 0.5691, 0.2767],\n","                [0.4101, 0.1520, 0.3140, 0.5707],\n","                [0.5691, 0.2645, 0.3615, 0.4411],\n","                [0.3615, 0.3615, 0.3615, 0.3615],\n","                [0.0201, 0.0201, 0.0201, 0.0201],\n","                [0.0201, 0.1520, 0.2645, 0.5691],\n","                [0.1520, 0.2645, 1.4667, 0.3140],\n","                [0.4667, 0.4667, 0.4667, 0.4667]\n","            ]), atol=1e-4\n","        )\n","\n","\n","    # Calculate policy, the greed policy by performing the argmax over the \n","    # action_value function.  Implement the policy function\n","    # as a matrix of size (num_states, num_actions).  \n","    \n","    ################################################\n","    ####### Place your implementation here #########\n","    ################################################\n","    policy_stable = True\n","\n","    for s in range(env.nS):\n","      old_action = policy[s].detach().clone()\n","      \n","      max_index = torch.argmax(action_value[s]).data # find index of max action's value\n","      policy[s][max_index] = 1 # update policy\n","\n","      if torch.allclose(old_action,policy[s]) == False: \n","        policy_stable = False\n","      if policy_stable: \n","        break\n","\n","    if debug:\n","        assert torch.allclose(\n","            policy,\n","            torch.tensor([\n","              [0., 0., 1., 0.],\n","              [0., 0., 0., 1.],\n","              [1., 0., 0., 0.],\n","              [0., 1., 0., 0.],\n","              [0., 0., 1., 0.],\n","              [1., 0., 0., 0.],\n","              [0., 0., 1., 0.],\n","              [1., 0., 0., 0.],\n","              [0., 0., 1., 0.],\n","              [0., 0., 0., 1.],\n","              [1., 0., 0., 0.],\n","              [1., 0., 0., 0.],\n","              [1., 0., 0., 0.],\n","              [0., 0., 0., 1.],\n","              [0., 0., 1., 0.],\n","              [1., 0., 0., 0.]\n","            ])\n","        )\n","\n","    return policy\n","\n","torch.manual_seed(0)\n","test = policy_improvement(env, torch.rand(env.nS), .9, True)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"433LIsDehsEk"},"source":["Now that we've implemente all the pieces, we put them together to form the full algorithm."]},{"cell_type":"code","metadata":{"id":"LYfujcuL3vco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615951975644,"user_tz":360,"elapsed":517,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}},"outputId":"7725ceed-0a89-44dc-9dde-355bf485313c"},"source":["policy = torch.ones([env.nS, env.nA]) / env.nA  # initialize arbitrary: 25% each action\n","state_value = torch.zeros(env.nS)  # state-value function.  initialize arbitrarily\n","\n","for n in range(10):\n","    # calculate new value function from the current policy    \n","    state_value = policy_evaluation(env, policy, gamma)\n","\n","    # calculate new policy from the value function\n","    policy = policy_improvement(env, state_value, gamma)\n","draw_policy(policy)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[['S' '>' 'v' '<']\n"," ['v' 'H' 'v' 'H']\n"," ['>' 'v' 'v' 'H']\n"," ['H' '>' '>' 'G']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WcYcXyZc_qHJ"},"source":["# Value Iteration\n","\n","Value iteration is quite similar to policy iteration, but we only do a single iteration on the policy evaluation.\n","\n","![image](https://drive.google.com/thumbnail?id=15HoyTTKbFLS58uyeZllUuopCu1JJfJry&sz=w800)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4pAhB0dHhvq","executionInfo":{"status":"ok","timestamp":1615951990199,"user_tz":360,"elapsed":265,"user":{"displayName":"Salinee Kingbaisomboon","photoUrl":"","userId":"02359635892756858441"}},"outputId":"f59f9c7c-bdef-4f70-be45-71e2ca580a50"},"source":["policy = torch.ones([env.nS, env.nA]) / env.nA\r\n","state_value = torch.zeros([env.nS])\r\n","theta=1e-8\r\n","\r\n","def action_value_one_iter_f(env, state_value, s, gamma=1):\r\n","    action_value = torch.zeros(env.nA)\r\n","    for a in range(env.nA):\r\n","        for prob, next_state, reward, done in env.P[s][a]:\r\n","            action_value[a] += prob * (reward + gamma * state_value[next_state])\r\n","    return action_value\r\n","\r\n","while True:\r\n","  state_value_previous = state_value.detach().clone()\r\n","\r\n","  # calculate state_value, the state-value function update iteration above.  \r\n","\r\n","  # (hint: you've already implemented the action_value calculation)\r\n","\r\n","  ################################################\r\n","  ####### Place your implementation here #########\r\n","  ################################################\r\n","  delta = 0\r\n","  for s in range(env.nS):\r\n","    v = state_value[s].detach().clone()\r\n","    state_value[s] = max(action_value_one_iter_f(env, state_value, s, gamma))\r\n","    delta = max(delta,abs(v-state_value[s]))\r\n","\r\n","  if delta < theta:\r\n","    break\r\n","\r\n","  if not state_value_previous.sum():\r\n","      assert torch.allclose(\r\n","          state_value,\r\n","          torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\r\n","          atol=1e-4\r\n","      )\r\n","\r\n","  if torch.allclose(state_value_previous, state_value): \r\n","      break\r\n","\r\n","# Calculate policy, the optimal deterministic policy from the action_value \r\n","# function as described above\r\n","\r\n","# ################################################\r\n","# ####### Place your implementation here #########\r\n","# ################################################\r\n","policy = policy_improvement(env, state_value, gamma)\r\n","\r\n","assert torch.allclose(\r\n","    policy,\r\n","    torch.tensor([\r\n","        [0., 1., 0., 0.],\r\n","        [0., 0., 1., 0.],\r\n","        [0., 1., 0., 0.],\r\n","        [1., 0., 0., 0.],\r\n","        [0., 1., 0., 0.],\r\n","        [1., 0., 0., 0.],\r\n","        [0., 1., 0., 0.],\r\n","        [1., 0., 0., 0.],\r\n","        [0., 0., 1., 0.],\r\n","        [0., 1., 0., 0.],\r\n","        [0., 1., 0., 0.],\r\n","        [1., 0., 0., 0.],\r\n","        [1., 0., 0., 0.],\r\n","        [0., 0., 1., 0.],\r\n","        [0., 0., 1., 0.],\r\n","        [1., 0., 0., 0.]\r\n","      ]),\r\n","    atol=1e-4\r\n",")\r\n","\r\n","draw_policy(policy)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[['S' '>' 'v' '<']\n"," ['v' 'H' 'v' 'H']\n"," ['>' 'v' 'v' 'H']\n"," ['H' '>' '>' 'G']]\n"],"name":"stdout"}]}]}